---
title: "Paket_erstellen"
author: "Vivian Stehmans"
format: html
editor: visual
---
# Pseudo Code
```{r}

# Input: Datensatz
# - Y
# - Kovariablen (Integer) (mehr als Y und id)
# - id Variable

boostedrirt <- function(datensatz){
  
  checking_inputs(datensatz)
  # unabhängige Variable Y im Datensatz
  # eine id Variable
  # integer Kovariate mit p>0
  
  
  para_opt # <- Erstellen vom Parametergrid für Parametertuning
  for (para in 1:nrow(para_opt)) { # Iterieren über Tuningparameter
     
  cv_data <- cv_ziehen(datensatz) #Crossvalidation Folds ziehen
  for (i in 1:5){ # Iterieren über Cross-Folds
    f_iter # Schritt 1: f_0 für iter = 1
    while (iter <= itermax && improv_count < 5){ # Schritt 2: adaptierter EM Algorithmus
      
            train_data[[i]]$neg_grad <- berechne_neg_grad(train_data[[i]], f_train)
      cv_stichprobe[[i]] <- ziehen_stichprobe(train_data[[i]],sub_sample = sub_sam, col_sample = col_sam)
      para_data <- cv_stichprobe[[i]]$data
        
        # para_data sind die Daten die unseren colsample und subsample entsprechen
      para_data$temp_res <- rep(NA,nrow(para_data))
        
        cart_modell <- schaetze_cart(para_data)
        #cat(length(f_train),nrow(f_train[cv_stichprobe[[i]]$sample_indicies]))
        para_data$temp_res <- berechne_temp_residuen(para_data, cart_modell, f_train[cv_stichprobe[[i]]$sample_indicies])
        lmm_modell <- schaetze_lmm(para_data)
        random_intercepts_list[[i]][[iter]] <- ranef(lmm_modell)$id * 0.1
        
        #cat(nrow(test_data[[i]]),length(f_test))
        test_data[[i]]$f_iter_act <- berechne_aktuelle_vorhersage(test_data[[i]], lmm_modell, f_test, cart_modell)
        #cat(nrow(train_data[[i]]),length(f_train))
        train_data[[i]]$f_iter_act <- berechne_aktuelle_vorhersage(train_data[[i]], lmm_modell, f_train,cart_modell) # Aber müsste hier nicht paradata verwendet werden oder train?
        f_train <- train_data[[i]]$f_iter_act
        rmse_test <- berechne_rmse(test_data[[i]])
        rmse_train <- berechne_rmse(train_data[[i]])
        #print(rmse_train)
        
        if (iter > 1 && rmse_test >= min(rmse_results[rmse_results$fold == i,]$rmse_test)) {
          improv_count <- improv_count + 1}
        if(improv_count == 5){iter_final <- iter -5}
        if(iter == itermax){iter_final <- iter}
        
        rmse_results <- rbind(rmse_results,data.frame(
          fold = i,
          iter = iter,
          colsample = col_sam,
          subsample = sub_sam,
          rmse_test = rmse_test,
          rmse_train = rmse_train,
          final_iter = iter_final
        ))


        #print(rmse_test)
        iter <- iter + 1
        
      }

    
  }} # Ende der Para Schleife
  final_object<- berechne_finales_model(data,rmse_result = rmse_results)
  final_object$random_intercept <- random_intercepts_list
  class(final_object) <- "boosted_rirt"
  return(final_object)
}

      
  }
  
  
  
  final_parameter <- trainings_algorithmus(cv_datensatz, tuning_parameter_grid)
  # tuning_parameter_grid
  # final parameter = iter_final, final_tuning_parameter, modelle
  
  final_model <- finales_model(datensatz, )
  
  
  
  
}

checking_inputs <- function(datensatz){
  
}


  


```

# Pakete laden

```{r}
pacman::p_load(
  SAEforest, emdi, saeSim, ggplot2, tidyr, saeTrafo,
  dplyr, sae, furrr, rbenchmark, doParallel,
  foreach, randomForest, future.apply, lme4, tidyverse, caret,checkmate, usethis, testthat 
)
```

# Daten generieren

```{r}

set.seed(100)

# Parameter
Domains <- 50
pop_size <- rep(100, Domains)

# Hilfsfunktionen: Integer-Kovariablen und Fehlerterm
gen_X1Int <- function(dat, m = dat$muD, s = 4) {
  dat[["X1"]] <- as.integer(round(rnorm(nrow(dat), mean = m, sd = s)))
  return(dat)
}

gen_X2Int <- function(dat, m = dat$muD, s = 2) {
  dat[["X2"]] <- as.integer(round(rnorm(nrow(dat), mean = m, sd = s)))
  return(dat)
}

gen_myE <- function(dat, m = 0, s = 1000) {
  dat[["e"]] <- rnorm(nrow(dat), mean = m, sd = s)
  return(dat)
}

# Aufbau der Population
setup <- sim_base(data = base_id(nDomains = Domains, nUnits = pop_size)) %>%
  sim_gen(gen_generic(runif, min = -1, max = 1, groupVars = "idD", name = "muD")) %>%
  sim_gen(gen_X1Int) %>%
  sim_gen(gen_X2Int) %>%
  sim_gen(generator = gen_myE) %>%
  sim_gen_v(mean = 0, sd = 500) %>%
  sim_resp_eq(Y = 15000 - 500 * X1 - 500 * X2 + v + e)

# Eine Population erzeugen
Pop <- sim(setup, R = 1)[[1]]

# Spalte 'idD' in 'id' umbenennen
names(Pop)[names(Pop) == "idD"] <- "id"

# Negative Einkommen auf 0 setzen
Pop$Y[Pop$Y < 0] <- 0

# Nur relevante Spalten
Pop <- Pop[, c("id", "X1", "X2", "Y")]


```

```{r}
set.seed(123)

# Parameter
n_groups <- 40       # Anzahl Gruppen
n_per_group <- 100    # Beobachtungen pro Gruppe
n_total <- n_groups * n_per_group
p <- 10              # Anzahl integer-Kovariaten

# ID-Variable
id <- rep(1:n_groups, each = n_per_group)

# Integer-Kovariaten (X1 bis X10), alle zufällig aus diskreten Verteilungen
X1 <- sample(1:10, n_total, replace = TRUE)
X2 <- sample(0:20, n_total, replace = TRUE)
X3 <- sample(1:5, n_total, replace = TRUE)
X4 <- rpois(n_total, lambda = 4)
X5 <- rbinom(n_total, size = 1, prob = 0.5)
X6 <- sample(10:30, n_total, replace = TRUE)
X7 <- sample(-10:10, n_total, replace = TRUE)
X8 <- sample(0:3, n_total, replace = TRUE)
X9 <- rpois(n_total, lambda = 2)
X10 <- sample(1:100, n_total, replace = TRUE)

# Gruppenspezifischer zufälliger Intercept
group_effect <- rnorm(n_groups, mean = 6, sd = 3)
random_intercepts <- group_effect[id]

# Zielvariable Y mit linearer + nichtlinearer + Interaktion + Random Intercept + Fehler
Y <- 5 +
  2*X1 -
  1*X2 +
  0.5*X3^2 -
  0.8*X4 +
  3*X5 +
  0.2*X6 +
  1.5*X7 +
  2*X8 +
  0.5*X9 +
  0.01*X10 +
  random_intercepts +
  rnorm(n_total, sd = 5)

# Alles in Data Frame
sim_data <- data.frame(
  id = as.factor(id),
  X1 = as.integer(X1),
  X2 = as.integer(X2),
  X3 = as.integer(X3),
  X4 = as.integer(X4),
  X5 = as.integer(X5),
  X6 = as.integer(X6),
  X7 = as.integer(X7),
  X8 = as.integer(X8),
  X9 = as.integer(X9),
  X10 = as.integer(X10),
  Y = Y
)

# Vorschau
head(sim_data)

```

# New Data

```{r}
set.seed(321)  # neuer Seed

# Parameter
n_groups <- 40
n_per_group <- 100
n_total <- n_groups * n_per_group

# ID-Variable
id <- rep(1:n_groups, each = n_per_group)

# Integer-Kovariaten
X1 <- sample(1:10, n_total, replace = TRUE)
X2 <- sample(0:20, n_total, replace = TRUE)
X3 <- sample(1:5, n_total, replace = TRUE)
X4 <- rpois(n_total, lambda = 4)
X5 <- rbinom(n_total, size = 1, prob = 0.5)
X6 <- sample(10:30, n_total, replace = TRUE)
X7 <- sample(-10:10, n_total, replace = TRUE)
X8 <- sample(0:3, n_total, replace = TRUE)
X9 <- rpois(n_total, lambda = 2)
X10 <- sample(1:100, n_total, replace = TRUE)

# Gruppenspezifischer zufälliger Intercept
# Höhere Varianz, damit der Effekt im Modell "sichtbar" bleibt
group_intercepts <- rnorm(n_groups, mean = 4, sd = 8)
random_intercepts <- group_intercepts[id]

# Neue lineare Effektgrößen
Y <- -2 +                   # Grundwert
  0.5*X1 +                 
  3*X2 +                   
  0.8*X3 +                 
  -0.5*X4 +                
  1*X5 +                   
  0.05*X6 +                
  2*X7 +                   
  -1*X8 +                  
  1.5*X9 +                 
  0.005*X10 +              
  random_intercepts +     
  rnorm(n_total, sd = 3)   # kleineres Residual-Rauschen

# Alles in Data Frame
sim_data <- data.frame(
  id = as.factor(id),
  X1 = as.integer(X1),
  X2 = as.integer(X2),
  X3 = as.integer(X3),
  X4 = as.integer(X4),
  X5 = as.integer(X5),
  X6 = as.integer(X6),
  X7 = as.integer(X7),
  X8 = as.integer(X8),
  X9 = as.integer(X9),
  X10 = as.integer(X10),
  Y = Y
)

# Vorschau
head(sim_data)


```

# Funktionen

```{r}


checking_inputs <- function(data) {
  assert_data_frame(data, min.cols = 3, min.rows = ncol(data) + 1)
  # Ist ein Dataframe
  
  invisible(sapply(data %>% dplyr::select(-id) , assert_numeric))
  # alle Variablen numerisch
  
  assert_names(names(data), must.include = c("Y", "id"))
  # Überprüfen, ob 'data' Y erforderliche Spaltennamen enthält
  
}

ziehen_stichprobe <- function(data, col_sample, sub_sample){
  size_row <- nrow(data)*sub_sample
  size_col <- (ncol(data)-3)*col_sample
  if(sub_sample == 1){sample_row <- 1:nrow(data)
  }else{
  sample_row <- sample(nrow(data), size = size_row, replace = FALSE)}
  
  preds <- setdiff(names(data), c("Y","id","neg_grad"))
  sample_col_pred <- sample(preds, size = size_col)
  sample_col <- c(c("Y","id","neg_grad"), sample_col_pred)
  
  
  stichprobe <- list(
    data = data[sample_row, sample_col],
    subsample = sub_sample,
    colsample = col_sample,
    sample_indicies = sample_row
  )
  stichprobe
}


berechne_neg_grad <- function(data, f_iter){
  neg_grad <- data$Y-f_iter
  neg_grad
  }


schaetze_cart <- function(data){
  rpart::rpart(neg_grad~.,data %>%
  dplyr::select(-Y,-id,-temp_res), maxdepth = 3)}


berechne_temp_residuen <- function(data, cart, f_iter){
  data$pred <- predict(cart)
  temp_res <- data$Y - (f_iter +data$pred)
  temp_res
}



schaetze_lmm <- function(data){
  lmer(formula = temp_res ~ 1 + (1|id), data = data%>%
  dplyr::select(-Y, -neg_grad))
}


berechne_aktuelle_vorhersage <- function(data,lmm_model, f_iter,cart, alpha = 0.1){
  random_eff_id<- ranef(lmm_model)$id
  beta0<- fixef(lmm_model)[1]
  data$f_lmm_id<- random_eff_id[as.character(data$id),]
  data$pred <- predict(cart,data)
  f_iter_act <- f_iter+ alpha *(data$pred - beta0+ data$f_lmm_id)
}


berechne_rmse <- function(data){
  rmse <- sqrt(1/nrow(data)*sum((data$f_iter_act -data$Y)^2))
  rmse
}

berechne_finales_model <- function(data, rmse_result) {
  final_iter <- rmse_result[rmse_result$final_iter > 0, ]
  agg_final_iter <- aggregate(rmse_test ~ colsample + subsample + final_iter,
                              data = final_iter,
                              FUN = mean)
  final_parameter <- agg_final_iter[which.min(agg_final_iter$rmse_test), ]
  final_col_sample <- final_parameter$colsample
  final_sub_sample <- final_parameter$subsample
  final_final_iter <- final_parameter$final_iter
  gesamt_data <- data
  
  rmse_final <- data.frame(iter = integer(), rmse_gesamt = numeric())
  f_gesamt <- rep(mean(gesamt_data$Y), nrow(gesamt_data))
  for (j in 1:final_final_iter) {
    gesamt_data$neg_grad <- berechne_neg_grad(gesamt_data, f_gesamt)
    cv_stichprobe <- ziehen_stichprobe(gesamt_data,
                                       sub_sample = final_sub_sample,
                                       col_sample = final_col_sample)
    para_data <- cv_stichprobe$data
  
    
    # para_data sind die Daten die unseren colsample und subsample entsprechen
    para_data$temp_res <- rep(NA, nrow(para_data))
    
    cart_modell <- schaetze_cart(para_data)
    # #print(cart_modell)
    para_data$temp_res <- berechne_temp_residuen(para_data, cart_modell, f_gesamt[cv_stichprobe$sample_indicies])
    lmm_modell <- schaetze_lmm(para_data)
    gesamt_data$f_iter_act <- berechne_aktuelle_vorhersage(gesamt_data, lmm_modell, f_gesamt, cart_modell)
    f_gesamt <- gesamt_data$f_iter_act
    f_residuen <- gesamt_data$Y-gesamt_data$f_iter_act
    rmse_gesamt <- berechne_rmse(gesamt_data)
    
    if (j == final_final_iter) {
            final_residuen <- f_residuen
            final_predict <- f_gesamt
        }
    rmse_final <- rbind(rmse_final, data.frame(iter = j, rmse_gesamt = rmse_gesamt))
  }
  return(list(rmse_final = rmse_final,
                residuen = final_residuen,
                predict = final_predict,
              rmse_progress = rmse_result,
              final_col_sample = final_col_sample,
              final_sub_sample = final_sub_sample
              ))
}





```

# Aktuell

```{r}


#

boostedrirt <- function(data) {
  checking_inputs(data)
  
  rmse_results <- data.frame(
  fold = integer(),
  iter = integer(),
  colsample = numeric(),
  subsample = numeric(),
  rmse_train = numeric(),
  rmse_test = numeric(),
  final_iter = integer()
)
  para_opt <- expand.grid(
  subsample = c(1, 0.7),
  colsample = c(1, 0.6)
)
  for (para in 1:nrow(para_opt)) {
    sub_sam <- para_opt$subsample[para]
    col_sam <- para_opt$colsample[para]
  # Erzeuge 5 - CV Daten
  cv_data <- caret::createFolds(data$Y, k = 5)
  # Ziehe für alle CV Folds Stichproben
  train_data <- NULL
  test_data <- NULL
  cv_stichprobe <- NULL
  rmse <- NULL
  random_intercepts_list <- vector("list", length = 5)
  for (i in 1:5) {
    train_data[[i]] <- data[-cv_data[[i]], ]
    test_data[[i]] <- data[cv_data[[i]], ]
    itermax <- 100
    f_train <- rep(mean(train_data[[i]]$Y), nrow(train_data[[i]]))
    f_test  <- rep(mean(train_data[[i]]$Y), nrow(test_data[[i]])) # Startwert
    iter_final <- 0
    improv_count <- 0
    iter <- 1
    while (iter <= itermax && improv_count < 5) {
      train_data[[i]]$neg_grad <- berechne_neg_grad(train_data[[i]], f_train)
      cv_stichprobe[[i]] <- ziehen_stichprobe(train_data[[i]],sub_sample = sub_sam, col_sample = col_sam)
      para_data <- cv_stichprobe[[i]]$data
        
        # para_data sind die Daten die unseren colsample und subsample entsprechen
      para_data$temp_res <- rep(NA,nrow(para_data))
        
        cart_modell <- schaetze_cart(para_data)
        #cat(length(f_train),nrow(f_train[cv_stichprobe[[i]]$sample_indicies]))
        para_data$temp_res <- berechne_temp_residuen(para_data, cart_modell, f_train[cv_stichprobe[[i]]$sample_indicies])
        lmm_modell <- schaetze_lmm(para_data)
        random_intercepts_list[[i]][[iter]] <- ranef(lmm_modell)$id * 0.1
        
        #cat(nrow(test_data[[i]]),length(f_test))
        test_data[[i]]$f_iter_act <- berechne_aktuelle_vorhersage(test_data[[i]], lmm_modell, f_test, cart_modell)
        #cat(nrow(train_data[[i]]),length(f_train))
        train_data[[i]]$f_iter_act <- berechne_aktuelle_vorhersage(train_data[[i]], lmm_modell, f_train,cart_modell) # Aber müsste hier nicht paradata verwendet werden oder train?
        f_train <- train_data[[i]]$f_iter_act
        rmse_test <- berechne_rmse(test_data[[i]])
        rmse_train <- berechne_rmse(train_data[[i]])
        #print(rmse_train)
        
        if (iter > 1 && rmse_test >= min(rmse_results[rmse_results$fold == i,]$rmse_test)) {
          improv_count <- improv_count + 1}
        if(improv_count == 5){iter_final <- iter -5}
        if(iter == itermax){iter_final <- iter}
        
        rmse_results <- rbind(rmse_results,data.frame(
          fold = i,
          iter = iter,
          colsample = col_sam,
          subsample = sub_sam,
          rmse_test = rmse_test,
          rmse_train = rmse_train,
          final_iter = iter_final
        ))


        #print(rmse_test)
        iter <- iter + 1
        
      }

    
  }} # Ende der Para Schleife
  final_object<- berechne_finales_model(data,rmse_result = rmse_results)
  final_object$random_intercept <- random_intercepts_list
  class(final_object) <- "boosted_rirt"
  return(final_object)
}

rmse_1 <- boostedrirt(sim_data)

```

# Methoden

```{r}
print.boosted_rirt <- function(boosted_rirt_obj){
  max_iter<- max(boosted_rirt_obj$rmse_final$iter)
  final_rmse <- boosted_rirt_obj$rmse_final[boosted_rirt_obj$rmse_final$iter == max_iter,]$rmse_gesamt
  cat("Anzahl der Iterationen:",max_iter,"\n")
  cat("Finaler RMSE:", final_rmse,"\n")
}
print(rmse_1)

summary.boosted_rirt <- function(boosted_rirt_obj){
  cat("  Residuen\n"
      )
  print(summary(boosted_rirt_obj$residuen))
  ggplot2::ggplot(data.frame(residuen = boosted_rirt_obj$residuen), aes(sample = residuen))+
  geom_qq()+
  stat_qq_line(color = "red") +
  labs(
    title = "Q-Q-Plot der Residuen",
    x = "Theoretische Quantile",
    y = "Beobachtete Quantile"
  ) +
  theme_minimal()
}
summary(rmse_1)


plot.boosted_rirt <- function(boosted_rirt_obj){
  plot1 <- ggplot2::ggplot(data.frame(residuen= boosted_rirt_obj$residuen),aes(x = residuen))+
  geom_histogram(aes(y =after_stat(density)))+
  geom_density()+
  labs(title="Histogramm der finalen Residuen mit Dichtekurve",
       y= "Dichte",
       x = "Residuen")+
  theme_gray()
  
  opt_para <- data.frame(colsample = boosted_rirt_obj$final_col_sample, subsample = boosted_rirt_obj$final_sub_sample)
  rmse_2 <- boosted_rirt_obj$rmse_progress
rmse_best <- rmse_2[
  rmse_2$colsample == opt_para$colsample &
  rmse_2$subsample == opt_para$subsample, ]

plot2 <- ggplot2::ggplot(rmse_best, aes(x = iter)) +
  geom_line(aes(y = rmse_train, color = as.factor(fold), linetype = "Train")) +
  geom_line(aes(y = rmse_test,  color = as.factor(fold), linetype = "Test")) +
  labs(
    title = "RMSE-Verlauf über Iterationen für alle Folds",
    x = "Iteration",
    y = "RMSE",
    color = "Fold",
    linetype = "Daten"
  ) +
  theme_gray()+
  scale_x_continuous(breaks = seq(min(rmse_best$iter), max(rmse_best$iter), by = 1))

print(plot1)
print(plot2)
  
}

plot(rmse_1)


predict.boosted_rirt <- function(boosted_rirt_obj,newdata){}








```

```{r}




final_object <- berechne_finales_model(sim_data,rmse_1)
```
